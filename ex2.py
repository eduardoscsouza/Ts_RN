# -*- coding: utf-8 -*-
"""Trabalho Prático2 - Redes Neurais.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13_bFrKrMbfnGJeU1tLdxcckdWTcNZSbn

# REDES NEURAIS - RELATÓRIO EXERCÍCIO 2


## Grupo
- Eduardo Souza - 9293481
- Edresson Casanova - 11572715
- Marcela Prince Antunes - 11548673

**marcela**

O exercício 2 consiste na implementação de uma rede neural MLP com o algoritmo *Backpropagation* para resolver o problema do operador XOR, que não pode ser resolvido com Adaline. Além disso, será resolvido o caso do mapeamento identidade.

#Definições

###Saída de um Neurônio:

$f(net)$

$net = \sum_{i=1}^{N} [I_{i}*w_{i}] + \theta$ 

Onde $f$ é a função de ativação, $I_{i}$ e $w_{i}$ são a entrada (input) de índice $i$ e seu respectivo peso (weight) , $N$ é o tamanho da entrada e $\theta$ é o viés (*bias*).

---

###Função de perda (Loss):

$L = \sum_{i=1}^{N} \frac{(y_{i} - \hat{y}_{i})}{2}^2$

Onde $y_{i}$ e $\hat{y}_{i}$ são respectivamente a saída (output) da rede e a saída correta na dimensão $i$, e $N$ é o número de dimensões da saída.

---

###Regra Delta:

$w(t+1) = w(t) - \eta*\frac{\partial L}{\partial w(t)}$

Onde $w$ é o peso de um neurônio, $t$ é o índice da iteração do treinamento, e $\eta$ é a taxa de aprendizado. 

---
---

#Expansões
###Camada de Saída

A seguir os pesos serão representados por $w_{n, i}$, onde $n$ é o índice do neurônio da camada de saída e $i$ é o índice do peso dentro de um neurônio. O $\theta$ dos neurônios será considerado apenas como um peso cuja entrada é sempre $1$.

Como o resultado do neurônio $n$ da camada de saída só afeta a dimensão $n$ da saída, a derivada de $L$ em função de algum peso da camada de saída será $0$ para todos as dimensões diferentes de $n$. Portanto:

$\frac{\partial L}{\partial w_{n, i}} = \frac{\partial  \sum_{j=1}^{N} \frac{(y_{j} - \hat{y}_{j})}{2}^2}{\partial w_{n, i}}$

$= \frac{\partial  \frac{(y_{n} - \hat{y}_{n})}{2}^2}{\partial w_{n, i}} $

$= \frac{\partial  \frac{(y_{n} - \hat{y}_{n})}{2}^2}{\partial (y_{n} - \hat{y}_{n})} * \frac{\partial(y_{n} - \hat{y}_{n})}{\partial y_{n}} * \frac{\partial y_{n}}{\partial w_{n, i}}$

$= (y_{n} - \hat{y}_{n}) * 1 * \frac{\partial y_{n}}{\partial w_{n, i}}$

Note que $y_{n}$ nada mais é que $f(net)$ dos neurônios da camada de saída.

$= (f(net_{n}) - \hat{y}_{n}) * \frac{\partial f(net_{n})}{\partial w_{n, i}}$

Como exemplo, utilizarei a função sigmoidal como função de ativação.

$f(net) = \frac{1}{1 + e ^{-net}}$

$f'(net) = f(net) * (1 - f(net))$

Continuando a derivação:

$= (f(net_{n}) - \hat{y}_{n}) * \frac{\partial f(net_{n})}{\partial net_{n}} * \frac{\partial net_{n}}{\partial w_{n, i}}$

$= (f(net_{n}) - \hat{y}_{n}) * [f(net_{n}) * (1 - f(net_{n}))] * I_{i}$

Lembrando que a $I_{i}$ é a entrada de índice $i$. Portanto:

$\frac{\partial L}{\partial w_{n, i}} = (f(net_{n}) - \hat{y}_{n}) * [f(net_{n}) * (1 - f(net_{n}))] * I_{i}$

---

###Camada Escondida

A seguir os pesos serão representados por $w_{n, i}$, onde $n$ é o índice do neurônio da camada escondida e $i$ é o índice do peso dentro de um neurônio. O $\theta$ será tratado como anteriormente.

Como o resultado do neurônio $n$ da camada escondida afeta todas as dimensões da saída, não podemos fazer o mesmo que na última derivação. No entanto, pelas propriedades da derivada, podemos derivar cada elemento da somatória de $L$ individualmente e somar os resultados. Portanto:

$\frac{\partial L}{\partial w_{n, i}} = \frac{\partial  \sum_{j=1}^{N} \frac{(y_{j} - \hat{y}_{j})}{2}^2}{\partial w_{n, i}}$

$= \frac{\partial \frac{(y_{1} - \hat{y}_{1})}{2}^2}{\partial w_{n, i}} + \frac{\partial \frac{(y_{2} - \hat{y}_{2})}{2}^2}{\partial w_{n, i}} + ... + \frac{\partial \frac{(y_{N} - \hat{y}_{N})}{2}^2}{\partial w_{n, i}}$

Expandindo para um $j$ genérico, $j$ será um indicador de que se trata do neurônio $j$ da camada de saída, $n$ o mesmo para o neurônio da escondida e $i$ será o índice do peso. Inicialmente, a expansão será idêntica à anterior.

$\frac{\partial  \frac{(y_{j} - \hat{y}_{j})}{2}^2}{\partial w_{n, i}} = \frac{\partial  \frac{(f(net_{j}) - \hat{y}_{j})}{2}^2}{\partial w_{n, i}}$

$= \frac{\partial  \frac{(f(net_{j}) - \hat{y}_{j})}{2}^2}{\partial (f(net_{j}) - \hat{y}_{j})} * \frac{\partial(f(net_{j}) - \hat{y}_{j})}{\partial f(net_{j})} * \frac{\partial f(net_{j})}{\partial net_{j}} * \frac{\partial net_{j}}{\partial w_{n, i}}$

$= (f(net_{j}) - \hat{y}_{j}) * [f(net_{j}) * (1 - f(net_{j}))] * \frac{\partial net_{j}}{\partial w_{n, i}}$

Como a saída da camada escondida é a entrada da camada de saída:

$= (f(net_{j}) - \hat{y}_{j}) * [f(net_{j}) * (1 - f(net_{j}))] * \frac{\partial net_{j}}{\partial f(net_{n})} * \frac{\partial f(net_{n})}{\partial net_{n}} * \frac{\partial net_{n}}{\partial w_{n, i}}$

$= (f(net_{j}) - \hat{y}_{j}) * [f(net_{j}) * (1 - f(net_{j}))] * w_{j, n} * [f(net_{n}) * (1 - f(net_{n}))] * I_{i}$

Portanto, para o neurônio como um todo:

$\frac{\partial L}{\partial w_{n, i}} = \sum_{j=1}^{N} \{(f(net_{j}) - \hat{y}_{j}) * [f(net_{j}) * (1 - f(net_{j}))] * w_{j, n} * [f(net_{n}) * (1 - f(net_{n}))] * I_{i}\}$

### Módulos Externos e Funções Auxiliares
"""

import numpy as np
from sklearn.preprocessing import minmax_scale

# Adiciona 1 aos vetores. Utilizado para adicionar o bias
def append_one(x):
	aux = np.ones(x.shape[0]+1)
	aux[0:x.shape[0]] = x
	return aux

# Calcula a acuracia da MLP utilizando uma amostra de entrada e suas respectivas saidas esperadas.
def get_acc(dataset_x, dataset_y, hidden_layer, out_layer):
	acc = 0
	for i in range(dataset_x.shape[0]):
		out = feed_mlp(dataset_x[i], hidden_layer, out_layer)[0]
		if(np.argmax(out) == np.argmax(dataset_y[i])):
			acc += 1

	return acc/dataset_x.shape[0]

"""### Geração da Rede

A função abaixo cria as camadas da rede, que são basicamente vetores que armazenam os pesos de cada neurônio. Nesse exercício, foi usada apenas uma camada escondida (**hidden_layer**), além da camada de saída (**out_layer**). A função pode ser usada para geração de qualquer rede MLP, uma vez que os tamanhos da entrada, da camada escondida e da camada de saída são especificados como parâmetros.

Os pesos são inicializados com valores aleatórios entre -0.1 e 0.1, conforme especificado no exercício.
"""

# Essa função gera duas camadas, uma de entrada e outra de sáida.
def generate_mlp(input_size, hidden_size, out_size):
	hidden_layer = np.random.random_sample((hidden_size, input_size+1))#generate value 0 to 1
	out_layer = np.random.random_sample((out_size, hidden_size+1)) # generate value 0 to 1
	# normalizado entre -0.1 e 0.1 para atender os requisitos do trabalho
	hidden_layer = minmax_scale(hidden_layer, feature_range=(-0.1,0.1))
	out_layer = minmax_scale(out_layer, feature_range=(-0.1,0.1))
	return hidden_layer, out_layer

"""### Função de Ativação

Como função de ativação, g(), foi usada a função logística, por atender os requisitos de ser uma função contínua, estritamente crescente e limitada. No processo de aprendizado é utilizada a derivada da função g(). Ambas são definidas abaixo.
"""

# função logistica, também conhecida como função sigmoid
def log(x):
	return 1/(1 + np.exp(-x))
#derivada da função logistica
def derivate_log(x):
	return x * (1.0 - x)

"""### Treinamento

O treinamento consiste em atualizar o valor dos pesos das camada de saída e escondida utilizando o valor de saída esperado.

A função **fit_mlp** implementa o processo de treinamento. Os parâmetros são:

*   **dataset_x**: contém a amostra de treinamento;
*   **dataset_y**: contém a saída esperada para cada item da amostra de treinamento;
*   **hidden_layer**: pesos iniciais da camada escondida;
*   **out_layer**: pesos iniciais da camada de saída;
*   **thresh**: erro esperado;
*   **eta**: taxa de aprendizagem;
*   **max_it**: número máximo de iterações.

O resultado da função são dois vetores com os pesos atualizados das camada escondida e de saída.

O processo de aprendizado inicia-se com o cálculo da saída utilizando-se os pesos atuais de cada uma das camadas (linha 7).

Posteriormente, é calculado o erro (linha 9) utilizando-se a fórmula:
$$ E(w) = \sum_{p=1}^{N}E_p(w)$$

Onde $N$ é o número total de itens na amostra de treinamento e $E_p$ é o erro quadrático (linha 8), dado por:
$$ E_p = \sum_{j}(t_{pj}-y_{pj})^2$$

Além disso, são atualizados os pesos da camada de saída e da camada escondida.

Inicialmente, é calculado o delta da camada de saída (linha 18), utilizando a fórmula:
$$ \dfrac{\partial E_{p}}{\partial w_{ij}} = - \delta_{pj} * y_{pi} $$

Onde $ - \delta_{pj} $ é calculado (linha 16), de acordo com a fórmula:
$$ - \delta_{pj} = \dfrac{\partial E_{p}}{\partial v_{pj}}  = 2 * (t_{pj} - y_{pj}) * y_{pj} * (1 - y_{pj})$$

Depois, é calculado o delta da camada escondida (linha 27), com a fórmula:
$$ \dfrac{\partial E_{p}}{\partial w_{ij}} = \left(\sum_{k}(- \delta_{pk} * w_{kj}) * y_{pj} * (1 - y_{pj}) * y_{pi}\right) $$

Por fim, os pesos são atualizados (linhas 30 e 31), conforme a fórmula:
$$ w_{ji}(k+1) = w_{ji}(k) - \eta * \dfrac{\partial E_{p}(w)}{\partial w_{ji}} \Bigg|_{w_{k}} $$

Este processo se repete até que o erro seja menor do que o esperado, ou se chegue ao número máximo de iterações.
"""

def fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=10000):
	count = 0 # variavel que controla o número de iterações
	error = thresh+1 # inicia o erro com um valor > thresh para o treinamento nao parar
	while(error > thresh and count < max_it):
		error = 0
		for i in np.arange(dataset_x.shape[0]):
			network_out, hidden_out = feed_mlp(dataset_x[i], hidden_layer, out_layer)# realiza a ativação da rede
      ep = np.sum(np.square(network_out-dataset_y[i]))
			error += ep # calcula o erro da ultima camada

      # calculo do delta de  camada de saida
			aux_hidden_out = append_one(hidden_out)#adiciona o 1 para o bias
			aux_der = np.empty(out_layer.shape[0])
			der_out = np.empty(out_layer.shape)
			for oi in range(out_layer.shape[0]):
				aux_der[oi] = 2 * (network_out[oi]-dataset_y[i, oi]) * derivate_log(network_out[oi]) #  2 * erro é a derivada da função de perda mean square error (MSE)
				for oj in range(out_layer.shape[1]):
					der_out[oi, oj] = aux_der[oi] * aux_hidden_out[oj] 
			
      #calculo do delta na camada escondida utilizando o delta da camada de saida
			aux_dataset_x = append_one(dataset_x[i])
			der_hidden = np.empty(hidden_layer.shape)
			for hi in np.arange(hidden_layer.shape[0]):
				for hj in np.arange(hidden_layer.shape[1]):
					aux_der_sum = 0
					for oi in np.arange(out_layer.shape[0]):
						aux_der_sum += aux_der[oi] * out_layer[oi, hi] * derivate_log(hidden_out[hi]) * aux_dataset_x[hj]
					der_hidden[hi, hj] = aux_der_sum

			out_layer = out_layer - eta*der_out # atualiza os pesos da ultima camada
			hidden_layer = hidden_layer - eta*der_hidden # atualiza os pesos da  camada oculta

		error /= dataset_x.shape[0]
		count += 1
		#print("Current error:", error)

	return hidden_layer, out_layer

"""### Ativação da Rede

A ativação da rede consiste em calcular uma saída a partir de uma entrada, utilizando os pesos calculados durante o treinamento.

*A* função **feed_mlp** implementa este processo. O passo inicial é adicionar um *bias* à entrada (linha 4). Então é calculada a saída da camada escondida, conforme a fórmula abaixo:
$$ F(o_1, ..., o_j) = g(\sum_{i} w_{ij} x_i )$$
Na fórmula acima, g() é a função de ativação.

Este processo é repetido para a camada de saída, utilizando como entrada a saída da camada escondida, calculada anteriormente.
"""

# essa função ativa a rede
def feed_mlp(network_input, hidden_layer, out_layer):
  
	aux_network_input = append_one(network_input) # adicionas 1 na entrada para o bias
	hidden_out = log(np.sum(aux_network_input * hidden_layer, axis=1)) # multiplica os pesos ou seja ativa  a rede 

	aux_hidden_out = append_one(hidden_out)#adiciona 1 para o bias
	network_out = log(np.sum(aux_hidden_out * out_layer, axis=1)) # calcula a ativação

	return network_out, hidden_out

"""### Criação dos Datasets e Execução dos Testes

Para cada um dos problemas, XOR e mapeamento identidade, são executados os passos abaixo:
* criação das amostras de teste e saídas esperadas;
* criação da MLP, especificando o tamanho das camadas de acordo com o problema;
* treinamento da MLP;
* cálculo da acurácia, através da ativação da MLP para cada um dos itens da amostra de teste.

A alteração do valor da taxa de aprendizagem (**eta**) pode impactar na acurácia da MLP para o caso do mapeamento identidade. No código abaixo, para cada problema, são executados treinamentos com taxas de aprendizagem 0.1 e 0.5, para demonstrar essa possível diferença.
"""

dataset_x = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]])
dataset_y = np.asarray([[0], [1], [1], [0]])

hidden_layer, out_layer = generate_mlp(2, 2, 1)
hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=1000)
print("XOR Accuracy (eta = 0.1):", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))

hidden_layer, out_layer = generate_mlp(2, 2, 1)
hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=1000)
print("XOR Accuracy (eta = 0.5):", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))


dataset_x = np.identity(8)
dataset_y = dataset_x
print(dataset_x)

hidden_layer, out_layer = generate_mlp(8, 3, 8)
hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=1000)
print("8-Dim Identity Accuracy (eta = 0.1):", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))

hidden_layer, out_layer = generate_mlp(8, 3, 8)
hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.5, max_it=1000)
print("8-Dim Identity Accuracy (eta = 0.5):", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))

print()

dataset_x = np.identity(15)
dataset_y = dataset_x
print(dataset_x)

hidden_layer, out_layer = generate_mlp(15, 4, 15)
hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=1000)
print("15-Dim Identity Accuracy (eta = 0.1):", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))

hidden_layer, out_layer = generate_mlp(15, 4, 15)
hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.5, max_it=1000)
print("15-Dim Identity Accuracy (eta = 0.5):", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))