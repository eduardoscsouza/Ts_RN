{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabalho Prático2 - Redes Neurais.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_2jwFCHx1_n",
        "colab_type": "text"
      },
      "source": [
        "# REDES NEURAIS - RELATÓRIO EXERCÍCIO 2\n",
        "\n",
        "\n",
        "## Grupo\n",
        "- Eduardo Souza - 9293481\n",
        "- Edresson Casanova - 11572715\n",
        "- Marcela Prince Antunes - 11548673\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGxtLb5D3Kgt",
        "colab_type": "text"
      },
      "source": [
        "**marcela**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o0K9o400uL3",
        "colab_type": "text"
      },
      "source": [
        "O exercício 2 consiste na implementação de uma rede neural MLP com o algoritmo *Backpropagation* para resolver o problema do operador XOR, que não pode ser resolvido com Adaline. Além disso, será resolvido o caso do mapeamento identidade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPqPzQb7WVUK",
        "colab_type": "text"
      },
      "source": [
        "#Definições\n",
        "\n",
        "###Saída de um Neurônio:\n",
        "\n",
        "$f(net)$\n",
        "\n",
        "$net = \\sum_{i=1}^{N} [I_{i}*w_{i}] + \\theta$ \n",
        "\n",
        "Onde $f$ é a função de ativação, $I_{i}$ e $w_{i}$ são a entrada (input) de índice $i$ e seu respectivo peso (weight) , $N$ é o tamanho da entrada e $\\theta$ é o viés (*bias*).\n",
        "\n",
        "---\n",
        "\n",
        "###Função de perda (Loss):\n",
        "\n",
        "$L = \\sum_{i=1}^{N} \\frac{(y_{i} - \\hat{y}_{i})}{2}^2$\n",
        "\n",
        "Onde $y_{i}$ e $\\hat{y}_{i}$ são respectivamente a saída (output) da rede e a saída correta na dimensão $i$, e $N$ é o número de dimensões da saída.\n",
        "\n",
        "---\n",
        "\n",
        "###Regra Delta:\n",
        "\n",
        "$w(t+1) = w(t) - \\eta*\\frac{\\partial L}{\\partial w(t)}$\n",
        "\n",
        "Onde $w$ é o peso de um neurônio, $t$ é o índice da iteração do treinamento, e $\\eta$ é a taxa de aprendizado. \n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "#Expansões\n",
        "###Camada de Saída\n",
        "\n",
        "A seguir os pesos serão representados por $w_{n, i}$, onde $n$ é o índice do neurônio da camada de saída e $i$ é o índice do peso dentro de um neurônio. O $\\theta$ dos neurônios será considerado apenas como um peso cuja entrada é sempre $1$.\n",
        "\n",
        "Como o resultado do neurônio $n$ da camada de saída só afeta a dimensão $n$ da saída, a derivada de $L$ em função de algum peso da camada de saída será $0$ para todos as dimensões diferentes de $n$. Portanto:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_{n, i}} = \\frac{\\partial  \\sum_{j=1}^{N} \\frac{(y_{j} - \\hat{y}_{j})}{2}^2}{\\partial w_{n, i}}$\n",
        "\n",
        "$= \\frac{\\partial  \\frac{(y_{n} - \\hat{y}_{n})}{2}^2}{\\partial w_{n, i}} $\n",
        "\n",
        "$= \\frac{\\partial  \\frac{(y_{n} - \\hat{y}_{n})}{2}^2}{\\partial (y_{n} - \\hat{y}_{n})} * \\frac{\\partial(y_{n} - \\hat{y}_{n})}{\\partial y_{n}} * \\frac{\\partial y_{n}}{\\partial w_{n, i}}$\n",
        "\n",
        "$= (y_{n} - \\hat{y}_{n}) * 1 * \\frac{\\partial y_{n}}{\\partial w_{n, i}}$\n",
        "\n",
        "Note que $y_{n}$ nada mais é que $f(net)$ dos neurônios da camada de saída.\n",
        "\n",
        "$= (f(net_{n}) - \\hat{y}_{n}) * \\frac{\\partial f(net_{n})}{\\partial w_{n, i}}$\n",
        "\n",
        "Como exemplo, utilizarei a função sigmoidal como função de ativação.\n",
        "\n",
        "$f(net) = \\frac{1}{1 + e ^{-net}}$\n",
        "\n",
        "$f'(net) = f(net) * (1 - f(net))$\n",
        "\n",
        "Continuando a derivação:\n",
        "\n",
        "$= (f(net_{n}) - \\hat{y}_{n}) * \\frac{\\partial f(net_{n})}{\\partial net_{n}} * \\frac{\\partial net_{n}}{\\partial w_{n, i}}$\n",
        "\n",
        "$= (f(net_{n}) - \\hat{y}_{n}) * [f(net_{n}) * (1 - f(net_{n}))] * I_{i}$\n",
        "\n",
        "Lembrando que a $I_{i}$ é a entrada de índice $i$. Portanto:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_{n, i}} = (f(net_{n}) - \\hat{y}_{n}) * [f(net_{n}) * (1 - f(net_{n}))] * I_{i}$\n",
        "\n",
        "---\n",
        "\n",
        "###Camada Escondida\n",
        "\n",
        "A seguir os pesos serão representados por $w_{n, i}$, onde $n$ é o índice do neurônio da camada escondida e $i$ é o índice do peso dentro de um neurônio. O $\\theta$ será tratado como anteriormente.\n",
        "\n",
        "Como o resultado do neurônio $n$ da camada escondida afeta todas as dimensões da saída, não podemos fazer o mesmo que na última derivação. No entanto, pelas propriedades da derivada, podemos derivar cada elemento da somatória de $L$ individualmente e somar os resultados. Portanto:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_{n, i}} = \\frac{\\partial  \\sum_{j=1}^{N} \\frac{(y_{j} - \\hat{y}_{j})}{2}^2}{\\partial w_{n, i}}$\n",
        "\n",
        "$= \\frac{\\partial \\frac{(y_{1} - \\hat{y}_{1})}{2}^2}{\\partial w_{n, i}} + \\frac{\\partial \\frac{(y_{2} - \\hat{y}_{2})}{2}^2}{\\partial w_{n, i}} + ... + \\frac{\\partial \\frac{(y_{N} - \\hat{y}_{N})}{2}^2}{\\partial w_{n, i}}$\n",
        "\n",
        "Expandindo para um $j$ genérico, $j$ será um indicador de que se trata do neurônio $j$ da camada de saída, $n$ o mesmo para o neurônio da escondida e $i$ será o índice do peso. Inicialmente, a expansão será idêntica à anterior.\n",
        "\n",
        "$\\frac{\\partial  \\frac{(y_{j} - \\hat{y}_{j})}{2}^2}{\\partial w_{n, i}} = \\frac{\\partial  \\frac{(f(net_{j}) - \\hat{y}_{j})}{2}^2}{\\partial w_{n, i}}$\n",
        "\n",
        "$= \\frac{\\partial  \\frac{(f(net_{j}) - \\hat{y}_{j})}{2}^2}{\\partial (f(net_{j}) - \\hat{y}_{j})} * \\frac{\\partial(f(net_{j}) - \\hat{y}_{j})}{\\partial f(net_{j})} * \\frac{\\partial f(net_{j})}{\\partial net_{j}} * \\frac{\\partial net_{j}}{\\partial w_{n, i}}$\n",
        "\n",
        "$= (f(net_{j}) - \\hat{y}_{j}) * [f(net_{j}) * (1 - f(net_{j}))] * \\frac{\\partial net_{j}}{\\partial w_{n, i}}$\n",
        "\n",
        "Como a saída da camada escondida é a entrada da camada de saída:\n",
        "\n",
        "$= (f(net_{j}) - \\hat{y}_{j}) * [f(net_{j}) * (1 - f(net_{j}))] * \\frac{\\partial net_{j}}{\\partial f(net_{n})} * \\frac{\\partial f(net_{n})}{\\partial net_{n}} * \\frac{\\partial net_{n}}{\\partial w_{n, i}}$\n",
        "\n",
        "$= (f(net_{j}) - \\hat{y}_{j}) * [f(net_{j}) * (1 - f(net_{j}))] * w_{j, n} * [f(net_{n}) * (1 - f(net_{n}))] * I_{i}$\n",
        "\n",
        "Portanto, para o neurônio como um todo:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_{n, i}} = \\sum_{j=1}^{N} \\{(f(net_{j}) - \\hat{y}_{j}) * [f(net_{j}) * (1 - f(net_{j}))] * w_{j, n} * [f(net_{n}) * (1 - f(net_{n}))] * I_{i}\\}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXOyOB6EYG_E",
        "colab_type": "text"
      },
      "source": [
        "### Módulos Externos e Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AioWBlmQWaN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "# Adiciona 1 aos vetores. Utilizado para adicionar o bias\n",
        "def append_one(x):\n",
        "\taux = np.ones(x.shape[0]+1)\n",
        "\taux[0:x.shape[0]] = x\n",
        "\treturn aux\n",
        "\n",
        "# Calcula a acuracia da MLP utilizando uma amostra de entrada e suas respectivas saidas esperadas.\n",
        "def get_acc(dataset_x, dataset_y, hidden_layer, out_layer):\n",
        "\tacc = 0\n",
        "\tfor i in range(dataset_x.shape[0]):\n",
        "\t\tout = feed_mlp(dataset_x[i], hidden_layer, out_layer)[0]\n",
        "\t\tif(np.argmax(out) == np.argmax(dataset_y[i])):\n",
        "\t\t\tacc += 1\n",
        "\n",
        "\treturn acc/dataset_x.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSOfX0h3834Q",
        "colab_type": "text"
      },
      "source": [
        "### Geração da Rede\n",
        "\n",
        "A função abaixo cria as camadas da rede, que são basicamente vetores que armazenam os pesos de cada neurônio. Nesse exercício, foi usada apenas uma camada escondida (**hidden_layer**), além da camada de saída (**out_layer**). A função pode ser usada para geração de qualquer rede MLP, uma vez que os tamanhos da entrada, da camada escondida e da camada de saída são especificados como parâmetros.\n",
        "\n",
        "Os pesos são inicializados com valores aleatórios entre -0.1 e 0.1, conforme especificado no exercício."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIR-_naS89NC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Essa função gera duas camadas, uma de entrada e outra de sáida.\n",
        "def generate_mlp(input_size, hidden_size, out_size):\n",
        "\thidden_layer = np.random.random_sample((hidden_size, input_size+1))#generate value 0 to 1\n",
        "\tout_layer = np.random.random_sample((out_size, hidden_size+1)) # generate value 0 to 1\n",
        "\t# normalizado entre -0.1 e 0.1 para atender os requisitos do trabalho\n",
        "\thidden_layer = minmax_scale(hidden_layer, feature_range=(-0.1,0.1))\n",
        "\tout_layer = minmax_scale(out_layer, feature_range=(-0.1,0.1))\n",
        "\treturn hidden_layer, out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFXWEbC3YcN9",
        "colab_type": "text"
      },
      "source": [
        "### Função de Ativação\n",
        "\n",
        "Como função de ativação, g(), foi usada a função logística, por atender os requisitos de ser uma função contínua, estritamente crescente e limitada. No processo de aprendizado é utilizada a derivada da função g(). Ambas são definidas abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnlaOoxY7fNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# função logistica, também conhecida como função sigmoid\n",
        "def log(x):\n",
        "\treturn 1/(1 + np.exp(-x))\n",
        "#derivada da função logistica\n",
        "def derivate_log(x):\n",
        "\treturn x * (1.0 - x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhdk9hBn-YQx",
        "colab_type": "text"
      },
      "source": [
        "### Treinamento\n",
        "\n",
        "O treinamento consiste em atualizar o valor dos pesos das camada de saída e escondida utilizando o valor de saída esperado.\n",
        "\n",
        "A função **fit_mlp** implementa o processo de treinamento. Os parâmetros são:\n",
        "\n",
        "*   **dataset_x**: contém a amostra de treinamento;\n",
        "*   **dataset_y**: contém a saída esperada para cada item da amostra de treinamento;\n",
        "*   **hidden_layer**: pesos iniciais da camada escondida;\n",
        "*   **out_layer**: pesos iniciais da camada de saída;\n",
        "*   **thresh**: erro esperado;\n",
        "*   **eta**: taxa de aprendizagem;\n",
        "*   **max_it**: número máximo de iterações.\n",
        "\n",
        "O resultado da função são dois vetores com os pesos atualizados das camada escondida e de saída.\n",
        "\n",
        "O processo de aprendizado inicia-se com o cálculo da saída utilizando-se os pesos atuais de cada uma das camadas (linha 7).\n",
        "\n",
        "Posteriormente, é calculado o erro (linha 9) utilizando-se a fórmula:\n",
        "$$ E(w) = \\sum_{p=1}^{N}E_p(w)$$\n",
        "\n",
        "Onde $N$ é o número total de itens na amostra de treinamento e $E_p$ é o erro quadrático (linha 8), dado por:\n",
        "$$ E_p = \\sum_{j}(t_{pj}-y_{pj})^2$$\n",
        "\n",
        "Além disso, são atualizados os pesos da camada de saída e da camada escondida.\n",
        "\n",
        "Inicialmente, é calculado o delta da camada de saída (linha 18), utilizando a fórmula:\n",
        "$$ \\dfrac{\\partial E_{p}}{\\partial w_{ij}} = - \\delta_{pj} * y_{pi} $$\n",
        "\n",
        "Onde $ - \\delta_{pj} $ é calculado (linha 16), de acordo com a fórmula:\n",
        "$$ - \\delta_{pj} = \\dfrac{\\partial E_{p}}{\\partial v_{pj}}  = 2 * (t_{pj} - y_{pj}) * y_{pj} * (1 - y_{pj})$$\n",
        "\n",
        "Depois, é calculado o delta da camada escondida (linha 27), com a fórmula:\n",
        "$$ \\dfrac{\\partial E_{p}}{\\partial w_{ij}} = \\left(\\sum_{k}(- \\delta_{pk} * w_{kj}) * y_{pj} * (1 - y_{pj}) * y_{pi}\\right) $$\n",
        "\n",
        "Por fim, os pesos são atualizados (linhas 30 e 31), conforme a fórmula:\n",
        "$$ w_{ji}(k+1) = w_{ji}(k) - \\eta * \\dfrac{\\partial E_{p}(w)}{\\partial w_{ji}} \\Bigg|_{w_{k}} $$\n",
        "\n",
        "Este processo se repete até que o erro seja menor do que o esperado, ou se chegue ao número máximo de iterações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REMNRKsQ_n-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=10000):\n",
        "\tcount = 0 # variavel que controla o número de iterações\n",
        "\terror = thresh+1 # inicia o erro com um valor > thresh para o treinamento nao parar\n",
        "\twhile(error > thresh and count < max_it):\n",
        "\t\terror = 0\n",
        "\t\tfor i in np.arange(dataset_x.shape[0]):\n",
        "\t\t\tnetwork_out, hidden_out = feed_mlp(dataset_x[i], hidden_layer, out_layer)# realiza a ativação da rede\n",
        "      ep = np.sum(np.square(network_out-dataset_y[i]))\n",
        "\t\t\terror += ep # calcula o erro da ultima camada\n",
        "\n",
        "      # calculo do delta de  camada de saida\n",
        "\t\t\taux_hidden_out = append_one(hidden_out)#adiciona o 1 para o bias\n",
        "\t\t\taux_der = np.empty(out_layer.shape[0])\n",
        "\t\t\tder_out = np.empty(out_layer.shape)\n",
        "\t\t\tfor oi in range(out_layer.shape[0]):\n",
        "\t\t\t\taux_der[oi] = 2 * (network_out[oi]-dataset_y[i, oi]) * derivate_log(network_out[oi]) #  2 * erro é a derivada da função de perda mean square error (MSE)\n",
        "\t\t\t\tfor oj in range(out_layer.shape[1]):\n",
        "\t\t\t\t\tder_out[oi, oj] = aux_der[oi] * aux_hidden_out[oj] \n",
        "\t\t\t\n",
        "      #calculo do delta na camada escondida utilizando o delta da camada de saida\n",
        "\t\t\taux_dataset_x = append_one(dataset_x[i])\n",
        "\t\t\tder_hidden = np.empty(hidden_layer.shape)\n",
        "\t\t\tfor hi in np.arange(hidden_layer.shape[0]):\n",
        "\t\t\t\tfor hj in np.arange(hidden_layer.shape[1]):\n",
        "\t\t\t\t\taux_der_sum = 0\n",
        "\t\t\t\t\tfor oi in np.arange(out_layer.shape[0]):\n",
        "\t\t\t\t\t\taux_der_sum += aux_der[oi] * out_layer[oi, hi] * derivate_log(hidden_out[hi]) * aux_dataset_x[hj]\n",
        "\t\t\t\t\tder_hidden[hi, hj] = aux_der_sum\n",
        "\n",
        "\t\t\tout_layer = out_layer - eta*der_out # atualiza os pesos da ultima camada\n",
        "\t\t\thidden_layer = hidden_layer - eta*der_hidden # atualiza os pesos da  camada oculta\n",
        "\n",
        "\t\terror /= dataset_x.shape[0]\n",
        "\t\tcount += 1\n",
        "\t\t#print(\"Current error:\", error)\n",
        "\n",
        "\treturn hidden_layer, out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10-Eb_dDFfHj",
        "colab_type": "text"
      },
      "source": [
        "### Ativação da Rede\n",
        "\n",
        "A ativação da rede consiste em calcular uma saída a partir de uma entrada, utilizando os pesos calculados durante o treinamento.\n",
        "\n",
        "*A* função **feed_mlp** implementa este processo. O passo inicial é adicionar um *bias* à entrada (linha 4). Então é calculada a saída da camada escondida, conforme a fórmula abaixo:\n",
        "$$ F(o_1, ..., o_j) = g(\\sum_{i} w_{ij} x_i )$$\n",
        "Na fórmula acima, g() é a função de ativação.\n",
        "\n",
        "Este processo é repetido para a camada de saída, utilizando como entrada a saída da camada escondida, calculada anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtaWC7MtYdzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# essa função ativa a rede\n",
        "def feed_mlp(network_input, hidden_layer, out_layer):\n",
        "  \n",
        "\taux_network_input = append_one(network_input) # adicionas 1 na entrada para o bias\n",
        "\thidden_out = log(np.sum(aux_network_input * hidden_layer, axis=1)) # multiplica os pesos ou seja ativa  a rede \n",
        "\n",
        "\taux_hidden_out = append_one(hidden_out)#adiciona 1 para o bias\n",
        "\tnetwork_out = log(np.sum(aux_hidden_out * out_layer, axis=1)) # calcula a ativação\n",
        "\n",
        "\treturn network_out, hidden_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMPjTn-iZB2E",
        "colab_type": "text"
      },
      "source": [
        "### Criação dos Datasets e Execução dos Testes\n",
        "\n",
        "Para cada um dos problemas, XOR e mapeamento identidade, são executados os passos abaixo:\n",
        "* criação das amostras de teste e saídas esperadas;\n",
        "* criação da MLP, especificando o tamanho das camadas de acordo com o problema;\n",
        "* treinamento da MLP;\n",
        "* cálculo da acurácia, através da ativação da MLP para cada um dos itens da amostra de teste.\n",
        "\n",
        "A alteração do valor da taxa de aprendizagem (**eta**) pode impactar na acurácia da MLP para o caso do mapeamento identidade. No código abaixo, para cada problema, são executados treinamentos com taxas de aprendizagem 0.1 e 0.5, para demonstrar essa possível diferença."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn8r5B_ZY_NU",
        "colab_type": "code",
        "outputId": "00f738e6-12c1-42b9-ebcb-9d6354b44f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "dataset_x = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "dataset_y = np.asarray([[0], [1], [1], [0]])\n",
        "\n",
        "hidden_layer, out_layer = generate_mlp(2, 2, 1)\n",
        "hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=1000)\n",
        "print(\"XOR Accuracy (eta = 0.1):\", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))\n",
        "\n",
        "hidden_layer, out_layer = generate_mlp(2, 2, 1)\n",
        "hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=1000)\n",
        "print(\"XOR Accuracy (eta = 0.5):\", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))\n",
        "\n",
        "\n",
        "dataset_x = np.identity(8)\n",
        "dataset_y = dataset_x\n",
        "print(dataset_x)\n",
        "\n",
        "hidden_layer, out_layer = generate_mlp(8, 3, 8)\n",
        "hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=1000)\n",
        "print(\"8-Dim Identity Accuracy (eta = 0.1):\", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))\n",
        "\n",
        "hidden_layer, out_layer = generate_mlp(8, 3, 8)\n",
        "hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.5, max_it=1000)\n",
        "print(\"8-Dim Identity Accuracy (eta = 0.5):\", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))\n",
        "\n",
        "print()\n",
        "\n",
        "dataset_x = np.identity(15)\n",
        "dataset_y = dataset_x\n",
        "print(dataset_x)\n",
        "\n",
        "hidden_layer, out_layer = generate_mlp(15, 4, 15)\n",
        "hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.1, max_it=1000)\n",
        "print(\"15-Dim Identity Accuracy (eta = 0.1):\", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))\n",
        "\n",
        "hidden_layer, out_layer = generate_mlp(15, 4, 15)\n",
        "hidden_layer, out_layer = fit_mlp(dataset_x, dataset_y, hidden_layer, out_layer, thresh=0.01, eta=0.5, max_it=1000)\n",
        "print(\"15-Dim Identity Accuracy (eta = 0.5):\", get_acc(dataset_x, dataset_y, hidden_layer, out_layer))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XOR Accuracy (eta = 0.1): 1.0\n",
            "XOR Accuracy (eta = 0.5): 1.0\n",
            "\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "8-Dim Identity Accuracy (eta = 0.1): 0.625\n",
            "8-Dim Identity Accuracy (eta = 0.5): 1.0\n",
            "\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "15-Dim Identity Accuracy (eta = 0.1): 0.4\n",
            "15-Dim Identity Accuracy (eta = 0.5): 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}